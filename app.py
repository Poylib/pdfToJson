import re
from io import BytesIO
from urllib.parse import urlparse, parse_qs
import html
import hashlib
import zipfile

import streamlit as st
from openpyxl import load_workbook
from openpyxl.utils import get_column_letter
import requests


st.set_page_config(page_title="POSCO IP Agent - RAG JSONL ìƒì„±ê¸°", page_icon="ğŸ›¡ï¸", layout="wide")

st.title("ğŸ›¡ï¸ POSCO IP Agent - RAG JSONL ìƒì„±ê¸°")
st.caption("ì—‘ì…€ â†’ ë©”íƒ€/ì›ë¬¸ ìˆ˜ì§‘ â†’ RAG ìµœì  ì²­í¬ JSON ìë™ ìƒì„±")


def normalize_header(value: str) -> str:
    if value is None:
        return ""
    return str(value).strip().replace("\u00A0", " ")  # non-breaking space ì œê±°


def extract_url_from_cell(cell) -> tuple[str | None, str | None]:
    """ì…€ì—ì„œ URLì„ ì¶”ì¶œí•˜ê³ , ì¶”ì¶œ ë°©ì‹(source)ì„ í•¨ê»˜ ë°˜í™˜í•œë‹¤.
    ìš°ì„ ìˆœìœ„: ì‹¤ì œ í•˜ì´í¼ë§í¬ â†’ HYPERLINK ìˆ˜ì‹ â†’ ë³¸ë¬¸ ë‚´ URL í…ìŠ¤íŠ¸
    """
    # 1) ì‹¤ì œ í•˜ì´í¼ë§í¬ ì†ì„±
    try:
        if getattr(cell, "hyperlink", None) and getattr(cell.hyperlink, "target", None):
            return cell.hyperlink.target, "hyperlink"
    except Exception:
        pass

    # 2) HYPERLINK ìˆ˜ì‹ í˜•íƒœ: =HYPERLINK("url","text") ë˜ëŠ” ë¡œìº˜ì— ë”°ë¼ ; ì‚¬ìš© ê°€ëŠ¥ì„±
    try:
        if isinstance(cell.value, str) and cell.value.startswith("=") and "HYPERLINK" in cell.value.upper():
            # ì²« ë²ˆì§¸ ì¸ìë¡œ ì˜¤ëŠ” URL ì¶”ì¶œ (ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„)
            m = re.search(r'HYPERLINK\(("|\')(.*?)("|\')', cell.value, re.IGNORECASE)
            if m:
                return m.group(2), "formula"
    except Exception:
        pass

    # 3) ì¼ë°˜ í…ìŠ¤íŠ¸ì— URL í¬í•¨
    try:
        if isinstance(cell.value, str):
            m = re.search(r"https?://[^\s)\]\"']+", cell.value)
            if m:
                return m.group(0), "text"
    except Exception:
        pass

    return None, None


def _absolutize_urls(html: str, base: str = "https://sd.wips.co.kr/wipslink/") -> str:
    if not html:
        return html
    # ../path -> absolute
    html = re.sub(r'(src|href)=[\"\']\.\./([^\"\']+)[\"\']', rf"\\1=\"{base}\\2\"", html)
    # //img4.wipson.com -> https://img4.wipson.com
    html = re.sub(r'(src|href)=[\"\']//', r"\\1=\"https://", html)
    return html


def _normalize_description_html(raw_html: str) -> str:
    if not raw_html:
        return ""
    html = raw_html
    # ê°„ë‹¨ íƒœê·¸ ì •ê·œí™”
    replacements = [
        ("<invention-title>", "<h3>"),
        ("</invention-title>", "</h3>"),
        ("<technical-field>", "<div id=\"technical-field\">"),
        ("</technical-field>", "</div>"),
        ("<background-art>", "<div id=\"background-art\">"),
        ("</background-art>", "</div>"),
        ("<summary-of-invention>", "<div id=\"summary-of-invention\">"),
        ("</summary-of-invention>", "</div>"),
        ("<description-of-drawings>", "<div id=\"description-of-drawings\">"),
        ("</description-of-drawings>", "</div>"),
        ("<description-of-embodiments>", "<div id=\"description-of-embodiments\">"),
        ("</description-of-embodiments>", "</div>"),
        ("<citation-list>", "<div id=\"citation-list\">"),
        ("</citation-list>", "</div>"),
        ("<embodiments-example>", "<div id=\"embodiments-example\">"),
        ("</embodiments-example>", "</div>"),
    ]
    for src, dst in replacements:
        html = html.replace(src, dst)
    # p íƒœê·¸ ë‹¨ìˆœí™”
    html = re.sub(r"<p\s+[^>]*>", "<p>", html, flags=re.IGNORECASE)
    # ì´ë¯¸ì§€/ë§í¬ ì ˆëŒ€ê²½ë¡œí™”
    html = _absolutize_urls(html)
    return html


def fetch_wips_description(target_url: str) -> dict:
    """WIPS ìƒì„¸ë³´ê¸° URLì—ì„œ ë°œëª…ì˜ ì„¤ëª…(DS) ì „ì²´ë¥¼ ì–¸ì–´ë³„ë¡œ ìˆ˜ì§‘í•œë‹¤.
    ë°˜í™˜: { 'doc': {...meta}, 'descriptions': {langCd: html}, 'order': [lang list] }
    """
    session = requests.Session()
    session.verify = False
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
        "Accept-Language": "ko,en;q=0.9",
    }
    r = session.get(target_url, headers=headers, timeout=20)
    r.raise_for_status()

    # skey ìš°ì„  URL ì¿¼ë¦¬ì—ì„œ
    parsed = urlparse(target_url)
    qs = parse_qs(parsed.query)
    skey = qs.get("skey", [None])[0]
    # ctry ë° í¼ hidden ê°’ ë³´ì™„
    html = r.text
    if not skey:
        m = re.search(r'id="skey"\s+value=\"(\d+)\"', html)
        if m:
            skey = m.group(1)
    m_ctry = re.search(r'id=\"ctry\"\s+value=\"([A-Z]{2})\"', html)
    ctry = m_ctry.group(1) if m_ctry else "WO"
    if not skey:
        raise RuntimeError("skeyë¥¼ í˜ì´ì§€ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    post_url = "https://sd.wips.co.kr/wipslink/doc/docContJson.wips"
    data = {
        "skey": skey,
        "tabGb": "DS",
        "isAbEnable": "true",
        "isClEnable": "true",
        "isDsEnable": "true",
        "isAdEnable": "false",
        "isPsEnable": "false",
        "isJdEnable": "false",
        "isFtEnable": "false",
        "devDocType": "DS",
        "devDocCtry": ctry,
    }
    ajax_headers = {
        **headers,
        "Referer": target_url,
        "X-Requested-With": "XMLHttpRequest",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8; metatype=json",
    }
    j = session.post(post_url, headers=ajax_headers, data=data, timeout=30)
    j.raise_for_status()
    payload = j.json()

    # ---------- ë©”íƒ€ ë³´ê°•: êµ­ê°€/ë¬¸í—Œë²ˆí˜¸ ----------
    doc = payload.get("docPageCmmRsltVO") or {}
    cfg = payload.get("docPageConfigVO") or {}

    def infer_ctry_from_url(path: str) -> str | None:
        if "dkrdshtm" in path:
            return "KR"
        if "dusdshtm" in path:
            return "US"
        if "dwodshtm" in path:
            return "WO"
        if "djpdshtm" in path:
            return "JP"
        if "depdshtm" in path:
            return "EP"
        if "dcndshtm" in path:
            return "CN"
        return None

    meta_ctry = doc.get("ctry") or cfg.get("devDocCtry") or ctry or infer_ctry_from_url(parsed.path) or ""
    nation_code = doc.get("nationCode") or meta_ctry
    composed = " ".join(x for x in [nation_code, doc.get("mngNum"), doc.get("docKind")] if x)
    nation_text = doc.get("nationCodetext") or composed.strip()
    if not nation_text:
        m2 = re.search(r'<p\s+class=\"nation_codetext\">\s*([^<]+)\s*</p>', html)
        if m2:
            nation_text = m2.group(1).strip()

    # ---------- ë³¸ë¬¸/ì–¸ì–´ ìˆ˜ì§‘ ----------
    descs = {}
    order = []

    def guess_lang(raw_html: str) -> str:
        if not raw_html:
            return "US"
        kor = len(re.findall(r"[\uac00-\ud7af]", raw_html))
        jpn = len(re.findall(r"[\u3040-\u30ff]", raw_html))
        han = len(re.findall(r"[\u4e00-\u9fff]", raw_html))
        lat = len(re.findall(r"[A-Za-z]", raw_html))
        total = max(kor + jpn + han + lat, 1)
        if kor / total > 0.2:
            return "KR"
        if jpn / total > 0.2:
            return "JP"
        if han / total > 0.4 and kor == 0 and jpn == 0:
            return "CN"
        return "US"

    # ì–¸ì–´ë³„ ì›ë¬¸/ë²ˆì—­ì„ ë¶„ë¦¬ ì €ì¥
    descs_by_lang: dict[str, dict[str, list[str]]] = {}

    def add_desc(lang: str, kind: str, html_content: str):
        bucket = descs_by_lang.setdefault(lang, {"origin": [], "translation": []})
        bucket[kind].append(html_content)

    # ì›ë¬¸(descList)
    for item in (payload.get("descList") or []):
        raw_lang = (item.get("langCd") or "").upper()
        html_part_raw = item.get("dtlDesc") or ""
        html_part = _normalize_description_html(html_part_raw)
        html_part = _absolutize_urls(html_part)
        lang = (raw_lang or guess_lang(html_part_raw) or "UNK").upper()
        add_desc(lang, "origin", html_part)
        if lang not in descs:
            descs[lang] = html_part
            order.append(lang)

    # ë²ˆì—­(descTrnsList)
    for item in (payload.get("descTrnsList") or []):
        raw_lang = (item.get("langCd") or "").upper()
        html_part_raw = item.get("dtlDesc") or ""
        html_part = _normalize_description_html(html_part_raw)
        html_part = _absolutize_urls(html_part)
        lang = (raw_lang or guess_lang(html_part_raw) or "TRNS").upper()
        add_desc(lang, "translation", html_part)
        if lang not in descs and lang not in order:
            descs[lang] = html_part
            order.append(lang)

    # ë©”íƒ€ ë³´ê°• ê²°ê³¼ ë°˜ì˜
    if doc is None:
        doc = {}
    doc = {**doc}
    if meta_ctry:
        doc.setdefault("ctry", meta_ctry)
    if nation_text:
        doc.setdefault("nationCodetext", nation_text)
    if cfg.get("devDocCtry"):
        doc.setdefault("devDocCtry", cfg.get("devDocCtry"))

    return {"doc": doc, "descriptions": descs, "order": order, "descriptions_by_lang": descs_by_lang}


def _strip_tags(html: str) -> str:
    if not html:
        return ""
    # script/style ì œê±°
    html = re.sub(r"<script[\s\S]*?</script>", " ", html, flags=re.IGNORECASE)
    html = re.sub(r"<style[\s\S]*?</style>", " ", html, flags=re.IGNORECASE)
    # ì¤„ë°”ê¿ˆ ìœ ë„ íƒœê·¸ë¥¼ ê°œí–‰ìœ¼ë¡œ ë³€í™˜
    html = re.sub(r"<(br|p|li|tr|div)(\s+[^>]*)?>", "\n", html, flags=re.IGNORECASE)
    # ê¸°íƒ€ íƒœê·¸ ì œê±°
    text = re.sub(r"<[^>]+>", " ", html)
    # HTML ì—”í‹°í‹° ê°„ë‹¨ ì¹˜í™˜
    text = text.replace("&nbsp;", " ").replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
    # ê³µë°± ì •ë¦¬
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\n+", "\n", text)
    return text.strip()


def _select_preferred_lang_text(descriptions_by_lang: dict) -> tuple[str, str]:
    """ì–¸ì–´ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ í…ìŠ¤íŠ¸ì™€ ì–¸ì–´ì½”ë“œë¥¼ ì„ íƒí•œë‹¤.
    ìš°ì„ ìˆœìœ„: KR(í•œêµ­ì–´) â†’ EN/US(ì˜ì–´) â†’ ê·¸ ì™¸ ì²« ë²ˆì§¸ ì–¸ì–´
    ë°˜í™˜: (plain_text, selected_lang)
    """
    if not descriptions_by_lang:
        return "", ""

    # KR ìš°ì„ , ë‹¤ìŒ EN/US, ì´í›„ ì„ì˜ ì²« ì–¸ì–´
    preferred_langs = ["KR", "EN", "US"]
    for lang in preferred_langs:
        if lang in descriptions_by_lang:
            bucket = descriptions_by_lang.get(lang, {"origin": [], "translation": []})
            htmls = (bucket.get("origin", []) or []) + (bucket.get("translation", []) or [])
            text = _strip_tags("\n".join(htmls))
            return text, lang

    # ìœ„ ìš°ì„ ìˆœìœ„ì— ì—†ìœ¼ë©´, ì²« ë²ˆì§¸ í‚¤ ì‚¬ìš©
    first_lang = next(iter(descriptions_by_lang.keys()))
    bucket = descriptions_by_lang.get(first_lang, {"origin": [], "translation": []})
    htmls = (bucket.get("origin", []) or []) + (bucket.get("translation", []) or [])
    text = _strip_tags("\n".join(htmls))
    return text, first_lang


def _decode_entities_and_normalize(text: str) -> str:
    if not text:
        return ""
    # ìˆ«ì/ì´ë¦„ ì—”í‹°í‹° 2íšŒê¹Œì§€ ë””ì½”ë”© (ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ ë°©ì§€)
    out = html.unescape(text)
    out = html.unescape(out)
    # ì¤„ë°”ê¿ˆ ì •ë¦¬ ë° ê³µë°± ì •ê·œí™”
    out = out.replace("\r\n", "\n").replace("\r", "\n")
    # 3ê°œ ì´ìƒ ì—°ì† ê°œí–‰ â†’ 2ê°œë¡œ ì¶•ì†Œ
    out = re.sub(r"\n{3,}", "\n\n", out)
    # íƒ­ì„ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜ í›„ ë‹¤ì¤‘ ê³µë°± ì¶•ì†Œ(ê°œí–‰ì€ ìœ ì§€)
    out = out.replace("\t", " ")
    out = re.sub(r"[ \f\v]{2,}", " ", out)
    return out.strip()


def _extract_section_hint(text: str) -> str | None:
    if not text:
        return None
    nums = [int(m) for m in re.findall(r"\[(\d{4})\]", text)]
    if not nums:
        return None
    return f"[{min(nums):04d}]-[{max(nums):04d}]"


def _split_by_paragraph_numbers(text: str) -> list[tuple[str, str]]:
    """íŠ¹í—ˆ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë²ˆí˜¸ [0001], [0002] ë“±ìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤.
    ë°˜í™˜: [(paragraph_number, paragraph_text), ...]
    """
    if not text:
        return []

    # [0001], [0002] í˜•íƒœì˜ ë¬¸ë‹¨ ë²ˆí˜¸ë¡œ ë¶„ë¦¬
    pattern = r'(\[\d{4}\])'
    parts = re.split(pattern, text)

    paragraphs = []
    current_num = None

    for i, part in enumerate(parts):
        part = part.strip()
        if not part:
            continue

        # ë¬¸ë‹¨ ë²ˆí˜¸ì¸ì§€ í™•ì¸
        if re.match(r'^\[\d{4}\]$', part):
            current_num = part
        elif current_num:
            # ë¬¸ë‹¨ ë²ˆí˜¸ ë‹¤ìŒì— ì˜¤ëŠ” ë‚´ìš©
            paragraphs.append((current_num, part))
            current_num = None
        else:
            # ë¬¸ë‹¨ ë²ˆí˜¸ ì—†ì´ ì‹œì‘í•˜ëŠ” í…ìŠ¤íŠ¸ (ì œëª© ë“±)
            paragraphs.append(("", part))

    return paragraphs


def _chunk_text_with_overlap(text: str, target_chars: int = 4000, overlap_chars: int = 400) -> list[str]:
    """íŠ¹í—ˆ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë²ˆí˜¸ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í¬ ë¶„í• í•œë‹¤."""
    if not text:
        return []
    if len(text) <= target_chars:
        return [text]

    # ë¬¸ë‹¨ ë²ˆí˜¸ë¡œ ë¶„ë¦¬
    paragraphs = _split_by_paragraph_numbers(text)

    if not paragraphs:
        # ë¬¸ë‹¨ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
        return _chunk_text_simple(text, target_chars, overlap_chars)

    chunks: list[str] = []
    current_chunk: list[str] = []
    current_size = 0

    for para_num, para_text in paragraphs:
        # ë¬¸ë‹¨ ë²ˆí˜¸ + í…ìŠ¤íŠ¸
        para_full = f"{para_num} {para_text}".strip() if para_num else para_text
        para_len = len(para_full)

        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
        if current_size + para_len > target_chars and current_chunk:
            # ì²­í¬ ì™„ì„±
            chunks.append("\n".join(current_chunk))

            # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ëª‡ ê°œ ë¬¸ë‹¨ ìœ ì§€
            overlap_paras = []
            overlap_size = 0
            for p in reversed(current_chunk):
                if overlap_size + len(p) <= overlap_chars:
                    overlap_paras.insert(0, p)
                    overlap_size += len(p)
                else:
                    break

            current_chunk = overlap_paras
            current_size = overlap_size

        current_chunk.append(para_full)
        current_size += para_len

    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append("\n".join(current_chunk))

    return chunks if chunks else [text]


def _chunk_text_simple(text: str, target_chars: int = 4000, overlap_chars: int = 400) -> list[str]:
    """ë¬¸ë‹¨ ë²ˆí˜¸ ì—†ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœ ë¶„í•  (í´ë°±ìš©)"""
    if not text:
        return []
    if len(text) <= target_chars:
        return [text]
    chunks: list[str] = []
    start = 0
    end = len(text)
    while start < end:
        cut = min(start + target_chars, end)
        # ë¬¸ë‹¨ ê²½ê³„ë¡œ ì‚´ì§ ë’¤ë¡œ ì´ë™
        boundary = text.rfind("\n\n", start + int(0.7 * target_chars), cut)
        if boundary != -1 and boundary > start:
            cut = boundary
        chunk = text[start:cut].strip()
        if chunk:
            chunks.append(chunk)
        if cut >= end:
            break
        start = max(cut - overlap_chars, start + 1)
    return chunks


def _build_chunk_records_for_doc(doc_id: str, base_metadata: dict, raw_text: str, lang: str) -> list[dict]:
    clean_text = _decode_entities_and_normalize(raw_text)
    # ì œëª© ì ‘ë‘ì‚¬
    title = (base_metadata or {}).get("title") or ""
    prefix = (title + "\n\n") if title else ""
    # ì²­í¬í™” (ë¬¸ë‹¨ ë²ˆí˜¸ ê¸°ì¤€)
    chunks = _chunk_text_with_overlap(clean_text, target_chars=4000, overlap_chars=400)
    total = len(chunks)
    out: list[dict] = []
    for i, c in enumerate(chunks, start=1):
        text_with_title = prefix + c if prefix else c

        # ë¬¸ë‹¨ ë²ˆí˜¸ ë²”ìœ„ ì¶”ì¶œ ([0001]-[0010] í˜•ì‹)
        section_hint = _extract_section_hint(c)

        rec = {
            "id": f"{doc_id}-chunk-{i:03d}of{total:03d}",
            "doc_id": doc_id,
            "text": text_with_title,
            "metadata": {
                **base_metadata,
                "lang": lang,
                "chunk_index": i,
                "chunk_total": total,
                "paragraph_range": section_hint or "",  # ë” ëª…í™•í•œ ì´ë¦„
            },
        }
        # ì„ íƒ: í…ìŠ¤íŠ¸ í•´ì‹œ
        try:
            rec["text_hash"] = hashlib.sha256(text_with_title.encode("utf-8")).hexdigest()
        except Exception:
            pass
        out.append(rec)
    # ë¹ˆ í…ìŠ¤íŠ¸ë¼ë©´ ìµœì†Œ 1ê±´ ìƒì„± ë°©ì§€
    return out

def _extract_pairs_from_li_html(li_html: str) -> list[dict]:
    """docSummaryInfo > li:first-child ë‚´ë¶€ì—ì„œ ë³´ì´ëŠ” Key-Value ìŒì„ ìµœëŒ€í•œ ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ì§‘í•œë‹¤.
    ë‹¤ì–‘í•œ ë§ˆí¬ì—…(dl/dt+dd, table th/td, span.title+span.value, í…ìŠ¤íŠ¸ ì½œë¡  êµ¬ë¶„)ì„ ì§€ì›í•œë‹¤.
    ë°˜í™˜ í˜•ì‹: [{"label": str, "value": str}, ...] (ìˆœì„œë¥¼ ë³´ì¡´í•˜ë ¤ ë…¸ë ¥)
    """
    if not li_html:
        return []
    pairs: list[dict] = []

    def add_pair(k: str, v: str):
        k = _strip_tags(k)
        v = _strip_tags(v)
        if k and v:
            pairs.append({"label": k, "value": v})

    # 1) dl/dt/dd íŒ¨í„´
    for m in re.findall(r"<dt[^>]*>([\s\S]*?)</dt>\s*<dd[^>]*>([\s\S]*?)</dd>", li_html, flags=re.IGNORECASE):
        add_pair(m[0], m[1])

    # 2) table tr (th/td ë˜ëŠ” td/td)
    for tr in re.findall(r"<tr[^>]*>([\s\S]*?)</tr>", li_html, flags=re.IGNORECASE):
        cells = re.findall(r"<(?:th|td)[^>]*>([\s\S]*?)</(?:th|td)>", tr, flags=re.IGNORECASE)
        if len(cells) >= 2:
            key = cells[0]
            val = " | ".join(cells[1:])
            add_pair(key, val)

    # 3) span.title + span.value í˜•íƒœ(í´ë˜ìŠ¤ëª… ìœ ì‚¬ ë§¤ì¹­)
    for m in re.findall(
        r"<span[^>]*class=\"[^\"]*(?:tit|title|key)[^\"]*\"[^>]*>([\s\S]*?)</span>\s*<span[^>]*class=\"[^\"]*(?:cont|val|value|data)[^\"]*\"[^>]*>([\s\S]*?)</span>",
        li_html,
        flags=re.IGNORECASE,
    ):
        add_pair(m[0], m[1])

    # 4) ì½œë¡ /í™”ì‚´í‘œ í¬í•¨ í…ìŠ¤íŠ¸ ë¼ì¸
    text_block = _strip_tags(li_html)
    for raw_line in [x.strip() for x in text_block.split("\n") if x.strip()]:
        # ìš°ì„ ìˆœìœ„: -->, ->, â†’, :, ï¼š
        if "-->" in raw_line:
            left, right = raw_line.split("-->", 1)
            add_pair(left, right)
            continue
        if "->" in raw_line:
            left, right = raw_line.split("->", 1)
            add_pair(left, right)
            continue
        if "â†’" in raw_line:
            left, right = raw_line.split("â†’", 1)
            add_pair(left, right)
            continue
        if ":" in raw_line or "ï¼š" in raw_line:
            sep = ":" if ":" in raw_line else "ï¼š"
            left, right = raw_line.split(sep, 1)
            add_pair(left, right)

    # ì¤‘ë³µ ì œê±°(ì•ì„  ìˆœì„œ ìš°ì„ )
    seen = set()
    deduped = []
    for p in pairs:
        key = (p["label"], p["value"])
        if key in seen:
            continue
        seen.add(key)
        deduped.append(p)
    return deduped


def _normalize_date(value: str) -> str:
    if not value:
        return ""
    m = re.search(r"(\d{4})[.\-/](\d{2})[.\-/](\d{2})", value)
    if m:
        return f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
    return value.strip()


def _slugify_key(text: str) -> str:
    t = re.sub(r"\s+", " ", text).strip().lower()
    t = re.sub(r"[()\[\]{}]+", " ", t)
    t = re.sub(r"[^a-z0-9]+", "_", t)
    t = re.sub(r"_+", "_", t).strip("_")
    return t or "field"


def _map_label_to_key(label: str) -> str | None:
    base = re.sub(r"\s+", "", _strip_tags(label)).lower()
    patterns = [
        (r"ìƒíƒœì •ë³´|status", "legal_status"),
        (r"ìµœì¢…ì²˜ë¶„|ê²°ì •|ë“±ë¡ê²°ì •|decision", "decision"),
        (r"ë“±ë¡ë²ˆí˜¸|ë¬¸í—Œë²ˆí˜¸|grantno|docnumber|documentnumber", "doc_number"),
        (r"ë°œí–‰ì¼|ê³µê³ ì¼|issue|gazette", "issue_date"),
        (r"ê³µê°œë²ˆí˜¸|publication", "publication_number"),
        (r"ê´€ë ¨íŠ¹í—ˆ|related", "related_patents"),
        (r"ì¶œì›ë²ˆí˜¸|application", "application_number"),
        (r"ì›ë¬¸ìƒ?ì¶œì›ì¸", "applicants_original"),
        (r"ì¶œì›ì¸ëŒ€í‘œ|ì¶œì›ì¸\s*ëŒ€í‘œëª…|ì¶œì›ì¸\s*ëŒ€í‘œëª…í™”|ëŒ€í‘œìëª…", "applicant_representative"),
        (r"ì¶œì›ì¸|applicant", "applicants"),
        (r"í˜„ì¬ê¶Œë¦¬ìëŒ€í‘œ|ê¶Œë¦¬ìëŒ€í‘œ|í˜„ì¬ê¶Œë¦¬ì\s*ëŒ€í‘œëª…|í˜„ì¬ê¶Œë¦¬ì\s*ëŒ€í‘œëª…í™”", "assignee_representative"),
        (r"í˜„ì¬ê¶Œë¦¬ì|ê¶Œë¦¬ì|assignee", "assignees"),
        (r"ì¶œì›íˆìŠ¤í† ë¦¬|íƒ€ì„ë¼ì¸|history", "timeline"),
    ]
    for pat, key in patterns:
        if re.search(pat, base):
            return key
    return None


def _split_list_like(text: str) -> list[str]:
    if not text:
        return []
    raw = _strip_tags(text)
    parts = [p.strip() for p in re.split(r"[;,/Â·ã†ä¸¨|]+", raw) if p.strip()]
    # ì¤‘ë³µ ì œê±° ìˆœì„œë³´ì¡´
    seen = set()
    out = []
    for p in parts:
        if p in seen:
            continue
        seen.add(p)
        out.append(p)
    return out


def structure_docsummary_pairs(pairs: list[dict]) -> dict:
    """ë¼ë²¨-ê°’ ë¦¬ìŠ¤íŠ¸ë¥¼ í‘œì¤€ í‚¤ë¥¼ ê°–ëŠ” JSONìœ¼ë¡œ ë³€í™˜í•œë‹¤."""
    fields: dict[str, object] = {}
    by_label: dict[str, str] = {}
    extras: dict[str, str] = {}
    for item in pairs:
        label = _strip_tags(item.get("label", ""))
        value = item.get("value", "")
        if not label:
            continue
        key = _map_label_to_key(label)
        clean_val = _strip_tags(value)

        if key in {"issue_date"}:
            val: object = _normalize_date(clean_val)
        elif key in {"applicants", "assignees", "related_patents"}:
            val = _split_list_like(clean_val)
        else:
            val = clean_val

        if key is None:
            extras[_slugify_key(label)] = val  # ì˜ë¯¸ë¥¼ ëª¨ë¥´ë©´ extrasë¡œ ë³´ì¡´
        else:
            if key in fields:
                # ê¸°ì¡´ ê°’ì´ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘í•©
                prev = fields[key]
                if isinstance(prev, list):
                    prev = prev + (val if isinstance(val, list) else [val])
                    fields[key] = [x for i, x in enumerate(prev) if x and x not in prev[:i]]
                else:
                    fields[key] = [prev] + (val if isinstance(val, list) else [val])
            else:
                fields[key] = val

        by_label[label] = clean_val

    return {"fields": fields, "by_label": by_label, "extras": extras}


def _extract_pairs_from_text_block(text: str) -> list[dict]:
    """ë¼ë²¨ í† ë§‰ë“¤ì´ í•œ ì¤„ì— ì´ì–´ì§€ëŠ” í˜•íƒœë¥¼ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤.
    ê·œì¹™: ì•Œë ¤ì§„ ë¼ë²¨ + (--> | -> | â†’ | : | ï¼š) + ê°’, ë‹¤ìŒ ë¼ë²¨ì´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ë¥¼ ê°’ìœ¼ë¡œ ê°„ì£¼.
    """
    if not text:
        return []
    raw = re.sub(r"\u00A0", " ", text)
    raw = re.sub(r"\s+", " ", raw).strip()

    # ì•Œë ¤ì§„ ë¼ë²¨ í›„ë³´(ë¹ˆë„ ìˆœìœ¼ë¡œ ë°°ì¹˜)
    labels = [
        "ìƒíƒœì •ë³´", "ìµœì¢…ì²˜ë¶„ë‚´ìš©", "ë“±ë¡ë²ˆí˜¸", "ê³µê³ ì¼", "ê³µê°œë²ˆí˜¸", "ê´€ë ¨íŠ¹í—ˆ", "ì¶œì›ë²ˆí˜¸",
        "ì¶œì›ì¸", "ì›ë¬¸ìƒ ì¶œì›ì¸", "ì¶œì›ì¸ ëŒ€í‘œëª…", "ì¶œì›ì¸ ëŒ€í‘œëª…í™”", "í˜„ì¬ê¶Œë¦¬ì", "í˜„ì¬ê¶Œë¦¬ì ëŒ€í‘œëª…",
        "í˜„ì¬ê¶Œë¦¬ì ëŒ€í‘œëª…í™”", "ì¶œì›íˆìŠ¤í† ë¦¬", "ì¶œì›ì¸ ëŒ€í‘œëª…ì¹­"
    ]
    # ë¼ë²¨ ì •ê·œì‹(ë¼ë²¨ì— ê³µë°± í—ˆìš©)
    label_regex = r"(?:" + r"|".join([re.escape(l).replace("\\ ", "\\s*") for l in labels]) + r")"
    delim = r"\s*(?:-->|->|â†’|:|ï¼š)\s*"
    pattern = re.compile(rf"({label_regex}){delim}")

    pairs: list[dict] = []
    matches = list(pattern.finditer(raw))
    for i, m in enumerate(matches):
        start_val = m.end()
        end_val = matches[i + 1].start() if i + 1 < len(matches) else len(raw)
        label = m.group(1)
        value = raw[start_val:end_val].strip()
        if label and value:
            pairs.append({"label": label, "value": value})
    return pairs


def fetch_wips_docsummary_first_li(target_url: str) -> dict:
    """ìƒì„¸ë³´ê¸° í˜ì´ì§€ì—ì„œ ul#docSummaryInfo ì˜ ì²« ë²ˆì§¸ li ë‚´ìš©ì„ íŒŒì‹±í•´ ë°˜í™˜í•œë‹¤.
    ë°˜í™˜: { 'url': url, 'skey': skey, 'pairs': [...], 'raw_html': str }
    """
    session = requests.Session()
    session.verify = False
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36",
        "Accept-Language": "ko,en;q=0.9",
    }
    r = session.get(target_url, headers=headers, timeout=20)
    r.raise_for_status()
    html = r.text

    parsed = urlparse(target_url)
    qs = parse_qs(parsed.query)
    skey = qs.get("skey", [None])[0]
    if not skey:
        m = re.search(r'id="skey"\s+value="(\d+)"', html)
        if m:
            skey = m.group(1)

    # docSummaryInfo ì˜ì—­ ì¶”ì¶œ â†’ ì²« ë²ˆì§¸ li
    # ì •ì  HTML ì‹œë„(ë”°ì˜´í‘œ/ê³µë°± ë³€í˜• í—ˆìš©)
    m_ul = re.search(r"<ul[^>]*id\s*=\s*[\"']docSummaryInfo[\"'][^>]*>([\s\S]*?)</ul>", html, flags=re.IGNORECASE)
    if not m_ul:
        # ë¹„ë™ê¸° ë¡œë“œ í´ë°±: docContJson.wipsì—ì„œ íƒ­ë³„ HTML ê²€ìƒ‰
        post_url = "https://sd.wips.co.kr/wipslink/doc/docContJson.wips"
        ajax_headers = {
            **headers,
            "Referer": target_url,
            "X-Requested-With": "XMLHttpRequest",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8; metatype=json",
        }

        # êµ­ê°€ ì½”ë“œ ì‹œë„(ì—†ì–´ë„ ì„œë²„ê°€ ì±„ì›€)
        m_ctry = re.search(r'id=\"ctry\"\s+value=\"([A-Z]{2})\"', html)
        ctry = m_ctry.group(1) if m_ctry else "WO"

        tab_candidates = ["OV", "AD", "DS", "CL", "AB", "PS", "JD", "FT"]

        def pick_first_html_with_summary(obj) -> str | None:
            if obj is None:
                return None
            stack = [obj]
            while stack:
                cur = stack.pop()
                if isinstance(cur, dict):
                    for v in cur.values():
                        stack.append(v)
                elif isinstance(cur, list):
                    for v in cur:
                        stack.append(v)
                elif isinstance(cur, str):
                    if "docSummaryInfo" in cur and "<ul" in cur:
                        return cur
            return None

        ul_html = None
        for tab in tab_candidates:
            try:
                data = {
                    "skey": skey or "",
                    "tabGb": tab,
                    "devDocType": tab,
                    "devDocCtry": ctry,
                    # ì—¬ëŸ¬ ì„¹ì…˜ í™œì„±í™” í”Œë˜ê·¸(ì„œë²„ê°€ ë¬´ì‹œí•´ë„ ë¬´ë°©)
                    "isAbEnable": "true",
                    "isClEnable": "true",
                    "isDsEnable": "true",
                    "isAdEnable": "true",
                    "isPsEnable": "true",
                    "isJdEnable": "true",
                    "isFtEnable": "true",
                }
                j = session.post(post_url, headers=ajax_headers, data=data, timeout=30)
                j.raise_for_status()
                payload = j.json()
                html_candidate = pick_first_html_with_summary(payload)
                if not html_candidate:
                    # JSON ì „ì²´ ì§ë ¬í™” í›„ ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ë¥¼ í•´ì œí•˜ë©° ê²€ìƒ‰
                    import json as _json
                    blob = _json.dumps(payload, ensure_ascii=False)
                    try:
                        blob_unesc = bytes(blob, "utf-8").decode("unicode_escape")
                    except Exception:
                        blob_unesc = blob
                    html_candidate = blob_unesc
                if html_candidate:
                    mu = re.search(r"<ul[^>]*id\s*=\s*[\"']docSummaryInfo[\"'][^>]*>([\s\S]*?)</ul>", html_candidate, flags=re.IGNORECASE)
                    if mu:
                        m_ul = mu
                        ul_html = mu.group(1)
                        break
            except Exception:
                continue

        if m_ul is None:
            raise RuntimeError("docSummaryInfo ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if 'ul_html' not in locals() or ul_html is None:
        ul_html = m_ul.group(1)
    m_li = re.search(r'<li[^>]*>([\s\S]*?)</li>', ul_html, flags=re.IGNORECASE)
    if not m_li:
        raise RuntimeError("docSummaryInfo ë‚´ ì²« ë²ˆì§¸ lië¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    li_html = m_li.group(1)

    pairs = _extract_pairs_from_li_html(li_html)
    if not pairs:
        # íƒœê·¸ ì œê±° í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ë¼ë²¨ ë°˜ë³µ íŒ¨í„´ìœ¼ë¡œ ì¬ì‹œë„
        pairs = _extract_pairs_from_text_block(_strip_tags(li_html))
    structured = structure_docsummary_pairs(pairs)
    return {"url": target_url, "skey": skey, "pairs": pairs, "structured": structured, "raw_html": li_html}


def _ensure_playwright_ready() -> tuple[bool, str | None]:
    """Playwright ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ í™•ì¸í•œë‹¤. ë¯¸ì„¤ì¹˜ ì‹œ ì•ˆë‚´ ë©”ì‹œì§€ ë°˜í™˜."""
    try:
        from playwright.sync_api import sync_playwright  # noqa: F401
    except Exception as e:
        return False, f"Playwright ë¯¸ì„¤ì¹˜: {e}. í„°ë¯¸ë„ì—ì„œ 'pip install -r requirements.txt' í›„ 'python -m playwright install chromium' ì‹¤í–‰ í•„ìš”."
    return True, None


def fetch_docsummary_with_browser(target_url: str, wait_ms: int = 8000) -> dict:
    """í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì €ë¡œ í˜ì´ì§€ë¥¼ ë Œë”ë§í•´ docSummaryInfoì˜ ì²« lië¥¼ ìˆ˜ì§‘í•œë‹¤."""
    ok, msg = _ensure_playwright_ready()
    if not ok:
        raise RuntimeError(msg or "Playwright not ready")

    from playwright.sync_api import sync_playwright

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36")
        page = context.new_page()
        page.set_default_timeout(max(wait_ms, 3000))
        page.goto(target_url, wait_until="domcontentloaded")
        # docSummaryInfoê°€ ë Œë”ë  ë•Œê¹Œì§€ ëŒ€ê¸°(íƒ­ ì „í™˜ ì§€ì—° ê³ ë ¤í•˜ì—¬ ì—¬ìœ ë¡­ê²Œ)
        try:
            page.wait_for_selector("ul#docSummaryInfo li", state="attached")
        except Exception:
            # ì¼ë¶€ í˜ì´ì§€ëŠ” íƒ­ ì „í™˜ í•„ìš” â†’ íƒ­ ì»¨í…Œì´ë„ˆ í›„ë³´ í´ë¦­ ì‹œë„
            try:
                # 'ë¬¸í—Œì „ì²´' ë˜ëŠ” 'ê°œìš”' íƒ­ ë²„íŠ¼ì„ í´ë¦­í•´ë³´ëŠ” ì‹œë„
                selectors = ["button[aria-controls='OV']", "#ov", "text=ê°œìš”", "text=ë¬¸í—Œì „ì²´"]
                for sel in selectors:
                    try:
                        page.locator(sel).first.click(timeout=1000)
                    except Exception:
                        pass
                page.wait_for_selector("ul#docSummaryInfo li", state="attached")
            except Exception:
                html_snap = page.content()
                browser.close()
                raise RuntimeError("ë Œë”ë§ëœ DOMì—ì„œ docSummaryInfoë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # ì›ì‹œ HTML ìŠ¤ëƒ…ìƒ·
        li_html = page.eval_on_selector("ul#docSummaryInfo li:nth-of-type(1)", "el => el.innerHTML")
        # DOMì—ì„œ ì§ì ‘ ë¼ë²¨-ê°’ì„ ì¶”ì¶œ(êµ¬ì¡° ì¸ì‹)
        pairs = page.evaluate(
            """
            () => {
              const root = document.querySelector('ul#docSummaryInfo li:nth-of-type(1)');
              const out = [];
              if (!root) return out;
              const push = (k, v) => { if (k && v) out.push({ label: k.trim(), value: v.trim() }); };

              // 1) dl/dt/dd
              root.querySelectorAll('dt').forEach(dt => {
                const dd = dt.nextElementSibling && dt.nextElementSibling.tagName.toLowerCase() === 'dd' ? dt.nextElementSibling : null;
                const k = dt.textContent || '';
                const v = dd ? dd.textContent : '';
                push(k, v);
              });

              // 2) table tr (th/td)
              root.querySelectorAll('tr').forEach(tr => {
                const cells = Array.from(tr.querySelectorAll('th,td')).map(c => (c.textContent || '').replace(/\s+/g,' ').trim()).filter(Boolean);
                if (cells.length >= 2) {
                  push(cells[0], cells.slice(1).join(' | '));
                }
              });

              // 3) .tit/.title/.key + next .cont/.val/.value/.data
              root.querySelectorAll('.tit, .title, .key').forEach(t => {
                const k = (t.textContent || '').trim();
                let sib = t.nextElementSibling;
                let v = '';
                if (sib && /cont|val|value|data/.test(sib.className)) v = sib.textContent || '';
                if (!v && sib && sib.querySelector) {
                  const cand = sib.querySelector('.cont, .val, .value, .data');
                  if (cand) v = cand.textContent || '';
                }
                if (k && v) push(k, v);
              });

              // 4) ì¼ë°˜ í–‰ì—ì„œ í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¿(:, â†’, ->, -->)
              Array.from(root.children || []).forEach(row => {
                const t = (row.textContent || '').replace(/\s+/g,' ').trim();
                const m = t.match(/^(.{1,40}?)(?:\s*(?:[:ï¼š]|â†’|-->|->)\s+)(.+)$/);
                if (m) push(m[1], m[2]);
              });

              // 5) ì¤‘ë³µ ì œê±°
              const seen = new Set();
              return out.filter(p => { const k = p.label + '|' + p.value; if (seen.has(k)) return false; seen.add(k); return true; });
            }
            """
        )

    if not pairs:
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¬ì‹œë„ (ë¸Œë¼ìš°ì €ê°€ ì—´ë ¤ìˆëŠ” ë™ì•ˆ ì‹¤í–‰)
        try:
            li_text = page.inner_text("ul#docSummaryInfo li:nth-of-type(1)")  # type: ignore
        except Exception:
            li_text = ""
        if li_text:
            pairs = _extract_pairs_from_text_block(li_text)
    # ì´ì œ ë¸Œë¼ìš°ì €ë¥¼ ë‹«ëŠ”ë‹¤
    try:
        browser.close()
    except Exception:
        pass
    structured = structure_docsummary_pairs(pairs)
    parsed = urlparse(target_url)
    qs = parse_qs(parsed.query)
    return {"url": target_url, "skey": qs.get("skey", [None])[0], "pairs": pairs, "structured": structured, "raw_html": li_html}


# ===== ë‹¨ì¼ í˜ì´ì§€: ì—‘ì…€ ì—…ë¡œë“œ â†’ ìë™ íŒŒì´í”„ë¼ì¸ â†’ ë¯¸ë¦¬ë³´ê¸°/ë‹¤ìš´ë¡œë“œ =====
st.header("ì—‘ì…€ ì—…ë¡œë“œ â†’ ìë™ ì²˜ë¦¬")

# session_state ì´ˆê¸°í™”
if "processing_complete" not in st.session_state:
    st.session_state.processing_complete = False
if "chunks_records" not in st.session_state:
    st.session_state.chunks_records = []
if "uploaded_file_id" not in st.session_state:
    st.session_state.uploaded_file_id = None

uploaded = st.file_uploader("íŠ¹í—ˆ ì—‘ì…€ íŒŒì¼(.xlsx) ì—…ë¡œë“œ", type=["xlsx"], accept_multiple_files=False, key="single_flow_uploader")

# ìƒˆë¡œìš´ íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ìƒíƒœ ì´ˆê¸°í™”
if uploaded is not None:
    current_file_id = uploaded.file_id
    if st.session_state.uploaded_file_id != current_file_id:
        st.session_state.processing_complete = False
        st.session_state.chunks_records = []
        st.session_state.uploaded_file_id = current_file_id

# ì´ë¯¸ ì²˜ë¦¬ê°€ ì™„ë£Œëœ ê²½ìš° ê²°ê³¼ë§Œ í‘œì‹œ
if uploaded is not None and st.session_state.processing_complete:
    st.success("âœ… ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    st.divider()

    chunks_records = st.session_state.chunks_records

    st.subheader(f"ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (ì´ {len(chunks_records)}ê°œ ì¤‘ ìµœëŒ€ 10ê°œ)")

    if chunks_records:
        preview_count = min(10, len(chunks_records))
        for i in range(preview_count):
            chunk_id = chunks_records[i].get('chunk_id', 'N/A')
            doc_id = chunks_records[i].get('doc_id', 'N/A')
            title = chunks_records[i].get('metadata', {}).get('title', '')
            text_len = len(chunks_records[i].get('text', ''))
            chunk_idx = chunks_records[i].get('chunk_index', '?')
            chunk_total = chunks_records[i].get('chunk_total', '?')
            para_range = chunks_records[i].get('paragraph_range', '')

            label = f"{i+1}. {chunk_id}"
            if para_range:
                label += f" | ë¬¸ë‹¨ {para_range}"
            if title:
                label += f" | {title[:30]}{'...' if len(title) > 30 else ''}"
            label += f" | {text_len:,}ì"
            with st.expander(label, expanded=(i==0)):
                st.json(chunks_records[i], expanded=False)

    # JSON ë°°ì—´(ìƒ¤ë”©) ë‹¤ìš´ë¡œë“œ
    def _make_json_shards(records: list[dict], max_bytes: int = 10 * 1024 * 1024) -> list[str]:
        import json as _json
        shards: list[str] = []
        current: list[dict] = []
        current_bytes = 2
        for rec in records:
            try:
                rec_str = _json.dumps(rec, ensure_ascii=False)
            except Exception:
                continue
            rec_size = len(rec_str.encode("utf-8"))
            sep = 1 if current else 0
            if current and (current_bytes + rec_size + sep) > max_bytes:
                shards.append(_json.dumps(current, ensure_ascii=False))
                current = []
                current_bytes = 2
                sep = 0
            current.append(rec)
            current_bytes += rec_size + sep
        if current:
            shards.append(_json.dumps(current, ensure_ascii=False))
        return shards

    chunks_shards = _make_json_shards(chunks_records)

    if len(chunks_shards) == 1:
        st.download_button(
            label=f"ğŸ“¥ chunks.json ë‹¤ìš´ë¡œë“œ ({len(chunks_records)}ê°œ ì²­í¬)",
            data=chunks_shards[0].encode("utf-8"),
            file_name="patents.chunks.json",
            mime="application/json",
            type="primary",
        )
    else:
        st.info(f"ğŸ’¡ ê²°ê³¼ê°€ 10MBë¥¼ ì´ˆê³¼í•˜ì—¬ {len(chunks_shards)}ê°œ íŒŒì¼ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ZIP ì••ì¶• ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for i, shard in enumerate(chunks_shards, start=1):
                file_name = f"patents.chunks.part-{i:03d}.json"
                zip_file.writestr(file_name, shard.encode("utf-8"))

        zip_buffer.seek(0)
        st.download_button(
            label=f"ğŸ“¦ ì „ì²´ ë‹¤ìš´ë¡œë“œ (ZIP, {len(chunks_shards)}ê°œ íŒŒì¼)",
            data=zip_buffer.getvalue(),
            file_name="patents_chunks_all.zip",
            mime="application/zip",
            type="primary",
        )

        st.divider()
        st.caption("ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")

        # ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        cols = st.columns(min(3, len(chunks_shards)))
        for i, shard in enumerate(chunks_shards, start=1):
            col_idx = (i - 1) % 3
            with cols[col_idx]:
                st.download_button(
                    label=f"ğŸ“¥ Part {i}/{len(chunks_shards)}",
                    data=shard.encode("utf-8"),
                    file_name=f"patents.chunks.part-{i:03d}.json",
                    mime="application/json",
                    key=f"download_part_{i}",
                )

    st.stop()

# ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš° í¬ë¡¤ë§ ì‹¤í–‰
if uploaded is not None and not st.session_state.processing_complete:
    progress = st.progress(0)
    with st.status("íŒŒì´í”„ë¼ì¸ ì‹œì‘", expanded=True) as status:
        try:
            # 1) íŒŒì¼ ë¡œë“œ
            st.write("1) íŒŒì¼ ë¡œë“œ")
            data = uploaded.read()
            progress.progress(5)

            # 2) ì—‘ì…€ íŒŒì‹± ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            st.write("2) ì—‘ì…€ íŒŒì‹± ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘")
            wb = load_workbook(filename=BytesIO(data), data_only=False, read_only=False)
            ws = wb.active
            progress.progress(20)

            # 'ì¶œì›ë²ˆí˜¸' í—¤ë” íƒìƒ‰(ë§í¬ ë³´ê´€ ì—´)
            target_col_idx = None
            header_row_idx = None
            matched_header_value = None
            search_rows = min(ws.max_row, 50)
            candidates = []
            for r in range(1, search_rows + 1):
                row_values = [normalize_header(c.value) for c in ws[r]]
                for c_idx, v in enumerate(row_values, start=1):
                    if not v:
                        continue
                    if v == "ì¶œì›ë²ˆí˜¸":
                        target_col_idx = c_idx
                        header_row_idx = r
                        matched_header_value = v
                        break
                    if "ì¶œì›ë²ˆí˜¸" in v:
                        candidates.append((c_idx, r, v))
                if target_col_idx is not None:
                    break
            if target_col_idx is None and candidates:
                target_col_idx, header_row_idx, matched_header_value = candidates[0]
            if target_col_idx is None:
                raise RuntimeError("'ì¶œì›ë²ˆí˜¸' í—¤ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì—‘ì…€ì˜ ë§í¬ ì—´ì„ 'ì¶œì›ë²ˆí˜¸'ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”.")

            # í—¤ë” ë§µ êµ¬ì„±
            headers = {}
            for col_idx, cell in enumerate(ws[header_row_idx], start=1):
                h = normalize_header(cell.value)
                if h:
                    headers[col_idx] = h

            # í–‰â†’URL ë§¤í•‘
            row_to_url = {}
            for r in range(header_row_idx + 1, ws.max_row + 1):
                cell = ws.cell(row=r, column=target_col_idx)
                url, source = extract_url_from_cell(cell)
                if url and "sd.wips.co.kr" in url:
                    row_to_url[r] = url.strip()

            # 3) ì›ë¬¸ ìˆ˜ì§‘ ë° RAG ì²­í¬ ìƒì„±
            st.write("3) íŠ¹í—ˆ ì›ë¬¸ ìˆ˜ì§‘ ë° RAG ì²­í¬ ìƒì„±")
            progress.progress(40)
            total_rows = max(ws.max_row - header_row_idx, 1)
            step_bar = st.progress(0, text="ìˆ˜ì§‘ ì§„í–‰ë¥ ")

            def _lang_to_bcp47_inline(lang: str) -> str:
                if not lang:
                    return "und"
                code = lang.upper()
                if code in {"KR", "KO"}:
                    return "ko"
                if code in {"US", "EN"}:
                    return "en"
                if code in {"JP", "JA"}:
                    return "ja"
                if code in {"CN", "ZH"}:
                    return "zh"
                return code.lower()

            def _make_doc_id_inline(ctry: str | None, publication_number: str | None, application_number: str | None, fallback: str) -> str:
                c = (ctry or "").strip().upper()
                pn = (publication_number or "").strip()
                an = (application_number or "").strip()
                if c and pn:
                    return f"{c}:{pn}"
                if c and an:
                    return f"{c}:{an}"
                return fallback

            chunks_records: list[dict] = []

            for idx, r in enumerate(range(header_row_idx + 1, ws.max_row + 1), start=1):
                row_data = {}
                for col_idx, header_name in headers.items():
                    cell = ws.cell(row=r, column=col_idx)
                    value = cell.value
                    if header_name in ["ì¶œì›ì¼", "ê³µê°œì¼", "ë“±ë¡ì¼"]:
                        value = _normalize_date(str(value)) if value else ""
                    elif value is not None:
                        value = str(value).strip()
                    else:
                        value = ""
                    row_data[header_name] = value

                if not any(v for v in row_data.values()):
                    step_bar.progress(min(int(100 * idx / total_rows), 100))
                    continue

                url = row_to_url.get(r)
                selected_text = ""
                selected_lang = ""
                ctry = ""
                publication_number = row_data.get("ê³µê°œë²ˆí˜¸", "")
                application_number = row_data.get("ì¶œì›ë²ˆí˜¸", "")
                title = row_data.get("ë°œëª…ì˜ ëª…ì¹­", "")

                if url:
                    try:
                        desc_out = fetch_wips_description(url)
                        by_lang = desc_out.get("descriptions_by_lang", {})
                        selected_text, selected_lang = _select_preferred_lang_text(by_lang)
                        ctry = (desc_out.get("doc", {}) or {}).get("ctry", "")
                    except Exception:
                        selected_text, selected_lang = "", ""

                doc_id_raw = application_number or publication_number or f"row_{r}"
                doc_id = _make_doc_id_inline(ctry, publication_number, application_number, doc_id_raw)
                processed_text = _decode_entities_and_normalize(selected_text)

                # ì²­í¬(chunks.json)
                if processed_text:
                    base_meta_for_chunk = {
                        "jurisdiction": (ctry or "").upper() or None,
                        "publication_number": publication_number,
                        "application_number": application_number,
                        "registration_number": row_data.get("ë“±ë¡ë²ˆí˜¸", ""),
                        "filing_date": row_data.get("ì¶œì›ì¼", ""),
                        "publication_date": row_data.get("ê³µê°œì¼", ""),
                        "registration_date": row_data.get("ë“±ë¡ì¼", ""),
                        "assignees": [row_data.get("ì¶œì›ì¸", "")] if row_data.get("ì¶œì›ì¸") else [],
                        "title": title,
                        "legal_status": row_data.get("ìƒíƒœì •ë³´", ""),
                        "wips_url": url or "",
                    }
                    chunks = _build_chunk_records_for_doc(doc_id, base_meta_for_chunk, processed_text, selected_lang)
                    # ìŠ¤í‚¤ë§ˆ ì •ê·œí™”
                    for ch in chunks:
                        if "id" in ch:
                            ch["chunk_id"] = ch.pop("id")
                        meta = ch.get("metadata", {}) or {}
                        lang_raw = meta.pop("lang", "")
                        ch["language"] = _lang_to_bcp47_inline(lang_raw)
                        if "chunk_index" in meta:
                            ch["chunk_index"] = meta.pop("chunk_index")
                        if "chunk_total" in meta:
                            ch["chunk_total"] = meta.pop("chunk_total")
                        if "paragraph_range" in meta:
                            ch["paragraph_range"] = meta.pop("paragraph_range")
                        ch["section"] = "description"
                        try:
                            ch["token_count"] = len(re.findall(r"\w+", ch.get("text", "")))
                        except Exception:
                            ch["token_count"] = 0
                    chunks_records.extend(chunks)

                step_bar.progress(min(int(100 * idx / total_rows), 100))

            # ê²°ê³¼ë¥¼ session_stateì— ì €ì¥
            st.session_state.chunks_records = chunks_records
            st.session_state.processing_complete = True

            progress.progress(100)
            status.update(label="ì²˜ë¦¬ ì™„ë£Œ", state="complete")
            st.success("âœ… í¬ë¡¤ë§ ì™„ë£Œ! í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ë©ë‹ˆë‹¤.")
            st.rerun()
        except Exception as e:
            status.update(label="ì˜¤ë¥˜ ë°œìƒ", state="error")
            st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # ë‹¨ì¼ í”Œë¡œìš°ë§Œ ë…¸ì¶œí•˜ê³  ê¸°ì¡´ íƒ­ UIëŠ” ë Œë”í•˜ì§€ ì•ŠìŒ
    st.stop()
