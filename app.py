import io
import json
from typing import Any, Dict, List, Optional

import streamlit as st
import pdfplumber

# íŠ¹í—ˆ ì „ì²˜ë¦¬ ëª¨ë“ˆ ë° zip ìƒì„±
from patent_processing import (
    chunks_to_jsonl,
)
import zipfile
import hashlib
from patent_processing import clean_text, split_sections, extract_claims, chunk_for_rag, extract_basic_metadata


# ë¶ˆí•„ìš”í•´ì§„ ì¼ë°˜ ë³€í™˜ ê´€ë ¨ í•¨ìˆ˜/ì½”ë“œë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.


def _ui_patent_converter() -> None:
    st.caption("íŠ¹í—ˆ ë¬¸ì„œì— ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ(ì„¹ì…˜/ì²­êµ¬í•­/ë©”íƒ€/ì²­í¬)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")

    col1, col2 = st.columns([3, 2])
    with col2:
        pretty_print = st.checkbox("ë¬¸ì„œ JSON ë“¤ì—¬ì“°ê¸°", value=True, key="pat_pretty")
        ensure_ascii = st.checkbox("ASCIIë§Œ ì‚¬ìš©", value=False, key="pat_ascii")

    uploaded_file = st.file_uploader("íŠ¹í—ˆ PDF ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=False, key="patent_uploader")
    if uploaded_file is None:
        st.info("íŠ¹í—ˆ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    file_bytes = uploaded_file.getvalue()

    # ì§„í–‰ ìƒí™© í‘œì‹œ: í˜ì´ì§€ ì¶”ì¶œ â†’ ì „ì²˜ë¦¬/ì„¹ì…˜ â†’ ì²­í¬í™”
    status = st.empty()
    progress = st.progress(0)
    try:
        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
            total_pages = len(pdf.pages)
            texts: List[str] = []
            for idx, page in enumerate(pdf.pages, start=1):
                status.write(f"í˜ì´ì§€ {idx}/{total_pages} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘â€¦")
                page_text = page.extract_text() or ""
                texts.append(page_text)
                progress.progress(min(70, int(idx / total_pages * 70)))

        status.write("ì „ì²˜ë¦¬ ë° ì„¹ì…˜ ë¶„ë¦¬ ì¤‘â€¦")
        cleaned = clean_text("\n".join(texts))
        sections = split_sections(cleaned)
        progress.progress(80)

        status.write("ì²­êµ¬í•­ ë¶„ì„ ë° ì²­í¬í™” ì¤‘â€¦")
        claims_text = ""
        for s in sections:
            if s.get("type") == "CLAIMS":
                claims_text = s.get("text", "")
                break
        claims = extract_claims(claims_text) if claims_text else []
        progress.progress(85)
        # 1) ì²­êµ¬í•­ ê¸°ë°˜ ì²­í¬
        claim_chunks = chunk_for_rag([], claims, target_tokens=600, overlap_tokens=80)
        progress.progress(90)
        # 2) ì„¹ì…˜ ê¸°ë°˜ ì²­í¬
        section_chunks = chunk_for_rag(sections, [], target_tokens=600, overlap_tokens=80)
        chunks = claim_chunks + section_chunks
        progress.progress(95)

        meta = extract_basic_metadata(cleaned)
        # ë¬¸ì„œ JSON êµ¬ì„±
        doc_id = hashlib.md5((uploaded_file.name + str(len(cleaned))).encode("utf-8")).hexdigest()
        document = {
            "doc_id": doc_id,
            "file_name": uploaded_file.name,
            "num_sections": len(sections),
            "num_claims": len(claims),
            "metadata": meta,
            "sections": sections,
            "claims": claims,
        }
        for ch in chunks:
            ch["doc_id"] = doc_id

        progress.progress(100)
        status.write("ì™„ë£Œ")
    except Exception as err:
        st.error(f"íŠ¹í—ˆ ë³€í™˜ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {err}")
        return

    st.success(f"íŠ¹í—ˆ ë³€í™˜ ì™„ë£Œ: ì„¹ì…˜ {document['num_sections']}ê°œ, ì²­êµ¬í•­ {document['num_claims']}ê°œ, ì²­í¬ {len(chunks)}ê°œ")

    indent_value: Optional[int] = 2 if pretty_print else None
    doc_json_str = json.dumps(document, ensure_ascii=ensure_ascii, indent=indent_value)
    from patent_processing import chunks_to_jsonl as _chunks_to_jsonl
    chunks_jsonl_str = _chunks_to_jsonl(chunks)

    tab_doc, tab_chunks = st.tabs(["ë¬¸ì„œ JSON", "ì²­í¬ JSONL"])
    with tab_doc:
        if pretty_print:
            st.code(doc_json_str, language="json")
        else:
            st.json(document)
    with tab_chunks:
        st.code(chunks_jsonl_str, language="json")

    base = uploaded_file.name.rsplit(".", 1)[0]
    st.download_button(
        label="ë¬¸ì„œ JSON ë‹¤ìš´ë¡œë“œ",
        data=doc_json_str.encode("utf-8"),
        file_name=f"{base}.patent.json",
        mime="application/json",
        key="pat_doc_dl",
    )
    st.download_button(
        label="ì²­í¬ JSONL ë‹¤ìš´ë¡œë“œ",
        data=chunks_jsonl_str.encode("utf-8"),
        file_name=f"{base}.chunks.jsonl",
        mime="application/json",
        key="pat_chunks_dl",
    )


def _ui_bulk_converter() -> None:
    st.caption("ì—¬ëŸ¬ íŠ¹í—ˆ PDFë¥¼ ë™ì‹œì— ë³€í™˜í•˜ì—¬ ZIPìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.")
    uploaded_files = st.file_uploader(
        "PDF ë‹¤ì¤‘ ì—…ë¡œë“œ",
        type=["pdf"],
        accept_multiple_files=True,
        key="bulk_uploader",
    )
    if not uploaded_files:
        st.info("ì—¬ëŸ¬ PDFë¥¼ ì„ íƒí•´ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    if st.button("ì¼ê´„ ë³€í™˜ ì‹œì‘", type="primary", key="bulk_start"):
        progress = st.progress(0)
        status = st.empty()

        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
            all_chunks: List[Dict[str, Any]] = []
            for idx, uf in enumerate(uploaded_files):
                status.write(f"ì²˜ë¦¬ ì¤‘: {uf.name} ({idx+1}/{len(uploaded_files)})")
                try:
                    # ë¬¸ì„œ ë³€í™˜ (ì „ì²´ í˜ì´ì§€)
                    with pdfplumber.open(io.BytesIO(uf.getvalue())) as pdf:
                        texts: List[str] = [p.extract_text() or "" for p in pdf.pages]
                    cleaned = clean_text("\n".join(texts))
                    sections = split_sections(cleaned)
                    claims_text = next((s.get("text", "") for s in sections if s.get("type") == "CLAIMS"), "")
                    claims = extract_claims(claims_text) if claims_text else []
                    claim_chunks = chunk_for_rag([], claims, target_tokens=600, overlap_tokens=80)
                    section_chunks = chunk_for_rag(sections, [], target_tokens=600, overlap_tokens=80)
                    chunks = claim_chunks + section_chunks
                    meta = extract_basic_metadata(cleaned)
                    doc_id = hashlib.md5((uf.name + str(len(cleaned))).encode("utf-8")).hexdigest()
                    doc = {
                        "doc_id": doc_id,
                        "file_name": uf.name,
                        "num_sections": len(sections),
                        "num_claims": len(claims),
                        "metadata": meta,
                        "sections": sections,
                        "claims": claims,
                    }
                    for ch in chunks:
                        ch["doc_id"] = doc_id
                except Exception as err:
                    error_msg = json.dumps({"file": uf.name, "error": str(err)}, ensure_ascii=False)
                    zf.writestr(f"errors/{uf.name}.error.json", error_msg)
                    continue

                base = uf.name.rsplit(".", 1)[0]
                zf.writestr(f"docs/{base}.patent.json", json.dumps(doc, ensure_ascii=False, indent=2))
                all_chunks.extend(chunks)

                progress.progress(int(((idx + 1) / len(uploaded_files)) * 100))

            zf.writestr("chunks/all.chunks.jsonl", chunks_to_jsonl(all_chunks))

        status.write("ì••ì¶• íŒŒì¼ ìƒì„± ì¤‘â€¦")
        zip_buffer.seek(0)
        st.download_button(
            label="ZIP ë‹¤ìš´ë¡œë“œ (ë¬¸ì„œ JSON + ì „ì²´ ì²­í¬ JSONL)",
            data=zip_buffer.getvalue(),
            file_name="patent_bulk_output.zip",
            mime="application/zip",
            key="bulk_zip_dl",
        )
        status.write("ì™„ë£Œ")


def main() -> None:
    st.set_page_config(page_title="Patent PDF â†’ JSON", page_icon="ğŸ“„", layout="wide")
    st.title("ğŸ“„ íŠ¹í—ˆ PDF â†’ JSON ë³€í™˜ê¸°")
    st.write("íŠ¹í—ˆ ë¬¸ì„œë¥¼ RAGì— ì í•©í•œ JSON/JSONLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")

    tab_patent, tab_bulk = st.tabs(["íŠ¹í—ˆ ë³€í™˜", "ëŒ€ëŸ‰ ë³€í™˜"])

    with tab_patent:
        _ui_patent_converter()
    with tab_bulk:
        _ui_bulk_converter()


if __name__ == "__main__":
    main()
